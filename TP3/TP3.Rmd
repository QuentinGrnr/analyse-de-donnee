---
title: "Analyse en Composantes Principales (ACP)"
author: "Analyse R"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, tidy = TRUE, comment = NA)
knitr::opts_chunk$set(tidy = TRUE, comment = NA)
```

# 1 Données départements : ACP et AFD
## 1.2 Analyse en Composantes Principales (ACP)
### Chargement des Bibliothèques
```{r}
options(repos = c(CRAN = "https://cloud. .org/"))
library(factoextra)
library(ggplot2)
```

### 1.2.1/2 Chargement des Données
```{r, fig.align = "center"}
dpt <- read.table("C:/Users/quent/Desktop/github/analyse-de-donnee/TP3/depart.dat",
                  header = FALSE, sep = "", strip.white = TRUE)

colnames(dpt) <- c("NumDep", "CodeDep", "CodeReg",
                   "TXCR", "ETRA", "URBR",
                   "JEUN", "AGE", "CHOM",
                   "AGRI", "ARTI", "CADR",
                   "EMPL", "OUVR", "PROF",
                   "FISC", "CRIM", "FE90")

dpt0 <- dpt[, c("TXCR", "ETRA", "URBR",
                "JEUN", "AGE", "CHOM",
                "AGRI", "ARTI", "CADR",
                "EMPL", "OUVR", "PROF",
                "FISC", "CRIM", "FE90")]
```

### 1.2.3 Réalisation de l'ACP
```{r, fig.align = "center"}
dpt.acp <- prcomp(dpt0, scale = TRUE)
```

### 1.2.4/5 Justification du nombre d'axes et représentations graphiques
#### Scree Plot des valeurs propres
```{r, fig.align = "center"}
plot(dpt.acp, type = "l", main = "Scree Plot (Valeurs Propres)")
```

#### Analyse des valeurs propres et variance expliquée
```{r}
eigenvalues <- dpt.acp$sdev^2
variance_explained <- eigenvalues / sum(eigenvalues) * 100
cumulative_variance <- cumsum(variance_explained)

data.frame(Axes = 1:length(eigenvalues),
           Eigenvalues = eigenvalues,
           Variance = variance_explained,
           CumulativeVariance = cumulative_variance)
```

### Interprétation des résultats
- **Le Scree Plot** montre que la courbe décroît fortement au début puis se stabilise après environ 4 ou 5 axes, indiquant qu’ils expliquent la majorité de la variance.
- **Les valeurs propres** indiquent que :
- La **première composante** explique **44.82%** de la variance.
- La **deuxième composante** en explique **22.22%**.
- La **troisième composante** en explique **8.86%**.
- La **quatrième composante** en explique **6.57%**.
- La **cinquième composante** en explique **4.03%**.
- En cumulant les cinq premières composantes, **on atteint 86.46% de la variance totale expliquée**.

### 1.2.6 Interprétation des axes
#### Contributions des variables aux axes principaux
```{r}
var <- get_pca_var(dpt.acp)
var$contrib  # Affiche la contribution des variables aux composantes principales
```


### Analyse des résultats des variables et individus
#### Cercle des corrélations (relation entre variables et axes)
```{r, fig.align = "center", fig.height = 3.5}
par(mfrow = c(2, 2))
fviz_pca_var(dpt.acp, axes = c(1, 2), col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_pca_var(dpt.acp, axes = c(3, 4), col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_pca_var(dpt.acp, axes = c(5, 6), col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
par(mfrow = c(1, 1))
```

#### Projection des individus sur plusieurs axes
```{r, fig.align = "center", fig.height = 3.5}
par(mfrow = c(2, 2))
fviz_pca_ind(dpt.acp, axes = c(1, 2), repel = TRUE, col.ind = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_pca_ind(dpt.acp, axes = c(3, 4), repel = TRUE, col.ind = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_pca_ind(dpt.acp, axes = c(5, 6), repel = TRUE, col.ind = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
par(mfrow = c(1, 1))
```

### Conclusion générale
- Le cos carré est une mesure de la qualité de la représentation des variables et des individus sur les axes.
- La visualisation des **corrélations des variables** et des **projections des individus** sur plusieurs paires d'axes permet d'avoir une compréhension plus fine des dimensions retenues.
- Les différents axes opposent divers aspects des départements :
- **Dim1 semble être une opposition entre les zones urbaines et rurales :**
  - URBR, CADR et FISC à gauche sont associés aux zones urbaines avec une forte fiscalité et un taux de criminalité plus élevé.
  - AGRI, AGE, OUVR à droite sont plus liés aux zones rurales avec une population plus âgée et plus d’ouvriers.
- **Dim2 semble opposer une structure démographique basée sur l’âge :**
  - JEUN et FE90 en bas indiquent une population jeune avec un fort taux de fécondité.
  - AGE et ARTI en haut indiquent une population plus âgée.
- **Dim 3 semble opposer les départements ayant un taux de criminalité élevé (TXCR, CRIM) à ceux ayant un taux de chômage plus important (CHOM).**
- **Dim 4 : Capte des différences liées aux professions intermédiaires (PROF) par rapport aux cadres et ouvriers/artisnts (CADR, OUVR).**
- **Dim 5 : Oppose les départements avec des revenus fiscaux élevés (FISC) à ceux avec une plus grande présence d’étrangers et de femmes en 1990 (ETRA, FE90).**

```
j'ai essayé d'interpréter le mieux possible bien que je ne sois pas sûr de tout
```

## 1.3 AFD du tableau de départements
```{r}
n<-rep(0,95); n[dpt[,3]=="NPC"]<-1
n[dpt[,3]=="Pic"]<-1; n[dpt[,3]=="HNo"]<-1
n[dpt[,3]=="ChA"]<-1; n[dpt[,3]=="Als"]<-2
n[dpt[,3]=="Lor"]<-2; n[dpt[,3]=="FrC"]<-2
n[dpt[,3]=="BNo"]<-3; n[dpt[,3]=="Bre"]<-3
n[dpt[,3]=="PaL"]<-3; n[dpt[,3]=="Cen"]<-4
n[dpt[,3]=="Bou"]<-4; n[dpt[,3]=="PoC"]<-5
n[dpt[,3]=="Lim"]<-5; n[dpt[,3]=="Auv"]<-6
n[dpt[,3]=="RhA"]<-6; n[dpt[,3]=="Aqu"]<-7
n[dpt[,3]=="MiP"]<-7; n[dpt[,3]=="LaR"]<-8
n[dpt[,3]=="PAC"]<-8; n[dpt[,3]=="Cor"]<-8
indice<-n>0
m<-subset(n,n>0)
xc<-1:95
ind<-xc[indice]
departements<-dpt0[ind,]
lbls<-c("Nord","Est","Ouest","CentreNord","CentreOuest","CentreEst",
        "SudEst","SudOuest")
partition<-factor(m,labels=lbls)
```

### On effectue l'AFD
```{r}
library(ade4)
departements.afd <- discrimin(dudi.pca(departements,scan=FALSE),
partition,scan=FALSE)
departements.afd
plot(departements.afd)
```

# 2 Données Iris : AFD et AFD décisionnelle

## Code
Voici le code pour l'analyse discriminante des données Iris :

```{r}
data(iris)
attach(iris)

# Discrimination en 2 dimensions #
plot(iris[,1],iris[,2],col=c("blue","red","green")[Species])
# Discrimination en 3 dimension #
library(rgl)
plot3d(iris[,1],iris[,2],iris[,3],col=c("blue","red","green")[Species],type="s")

# AFD
library(ade4)
iris.afd <- discrimin(dudi.pca(iris[,1:4],scan=FALSE),Species,scan=FALSE)
iris.afd
plot(iris.afd)
```

## Interprétation des valeurs propres :
- Le premier axe discriminant (0.9699) capte l’essentiel de la variance et sépare fortement les classes.
- Le deuxième axe (0.222) contribue faiblement mais aide à affiner la distinction entre versicolor et virginica.

## Interprétation des graphiques :
L'affichage contient plusieurs sous-plots expliquant la structure des données :

### (En haut à gauche) - Poids des variables (Canonical weights)
Petal.Width et Sepal.Width sont les variables les plus discriminantes. Ces variables expliquent comment chaque caractéristique influence la séparation des espèces.

### (En bas à gauche) - Cercle des cosinus (Cosinus des variables canoniques)
Sepal.Width et Sepal.Length influencent le premier axe. Ce graphique permet de comprendre comment les variables corrèlent avec les axes discriminants.

### (Centre) - Scores et classes des individus
Setosa est totalement séparée des autres classes. Versicolor et Virginica sont plus proches, ce qui suggère qu'elles partagent des caractéristiques similaires.

### (En bas à droite) - Scores des classes
Setosa est isolée, ce qui confirme qu'elle est bien distincte. Versicolor et Virginica sont plus proches, ce qui signifie qu'elles sont plus difficiles à discriminer.

### (En bas à gauche) - Valeurs propres (Eigenvalues)
Un histogramme montre l’importance des axes discriminants. L’axe 1 domine, ce qui confirme qu'une seule dimension suffit largement à séparer les espèces.

## AFD décisionnelle
```{r}
library(MASS)
train <- sample(1:150, 75)
table(Species[train])
iris.afd <- lda(Species ~ ., iris, prior = c(1,1,1)/3, subset = train)
iris.afd
pred <- predict(iris.afd, iris[-train, ])$class
table(Species[-train],pred)
```

### Interprétation de la matrice de confusion :
- Setosa est parfaitement classée (100% de bonnes prédictions).
- Versicolor et Virginica sont parfois confondues (3 erreurs chacune).
- Taux de bonne classification global : (30+21+18) / 75 = 69 / 75 = 92%
92% de bonne classification, ce qui est très bon pour un modèle LDA.


#### Pour réduire le taux d’erreur, on pourrait tester d’autres méthodes comme le SVM ou Random Forest. J'ai pu apprendre ces deux méthodes en Machine Learning en Erasmus

